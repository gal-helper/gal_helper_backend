from langchain_core.tools import tool
from app.core.langchain import langchain_manager
from app.core.config import config
from app.reranker.reranker import reranker
import logging
import json

logger = logging.getLogger(__name__)


@tool
async def retrieve_documents(query: str, k: int = 5) -> str:
    """
    æ£€ç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œè¿”å› JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å« items åˆ—è¡¨ã€‚
    """
    try:
        # å•è¡¨æ¨¡å¼ï¼šç›´æ¥è·å–å‘é‡å­˜å‚¨
        vectorstore = langchain_manager.get_vectorstore()
        
        # ğŸ›¡ï¸ æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦å¯ç”¨
        if vectorstore is None:
            logger.error("âŒ Vectorstore is None - ai_documents table not available or not initialized")
            return json.dumps({
                "items": [], 
                "count": 0,
                "error": "Knowledge base not initialized. Please import documents first."
            }, ensure_ascii=False)
        
        # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
        try:
            initial_k = k * 2 if getattr(config, 'RERANKER_ENABLED', False) else k
            docs = await vectorstore.asimilarity_search(query, k=initial_k)
        except AttributeError as e:
            logger.error(f"âŒ Vectorstore method error: {e}")
            return json.dumps({
                "items": [],
                "count": 0,
                "error": "Vectorstore interface error"
            }, ensure_ascii=False)

        if not docs:
            logger.debug(f"No documents found for query: {query}")
            return json.dumps({"items": [], "count": 0}, ensure_ascii=False)

        # é‡æ’åºå¤„ç†
        if getattr(config, 'RERANKER_ENABLED', False) and reranker.is_embedding_available() and len(docs) > 1:
            try:
                doc_contents = [doc.page_content for doc in docs]
                reranked = reranker.rerank_by_vector_cosine(query, doc_contents, top_k=k)

                new_docs = []
                used = set()
                for content, score in reranked:
                    for i, doc in enumerate(docs):
                        if i not in used and doc.page_content == content:
                            new_docs.append((doc, score))
                            used.add(i)
                            break

                if len(new_docs) == k:
                    docs = [d for d, _ in new_docs]
            except Exception as e:
                logger.warning(f"Vector cosine reranking failed: {e}")
                docs = docs[:k]
        elif getattr(config, 'RERANKER_ENABLED', False) and reranker.is_available() and len(docs) > 1:
            try:
                doc_contents = [doc.page_content for doc in docs]
                reranked = reranker.rerank(query, doc_contents, top_k=k)

                new_docs = []
                used = set()
                for content, _ in reranked:
                    for i, doc in enumerate(docs):
                        if i not in used and doc.page_content == content:
                            new_docs.append(doc)
                            used.add(i)
                            break

                if len(new_docs) == k:
                    docs = new_docs
            except Exception as e:
                logger.warning(f"CrossEncoder reranking failed: {e}")
                docs = docs[:k]
        else:
            docs = docs[:k]

        items = []
        for i, doc in enumerate(docs, 1):
            filename = doc.metadata.get("filename", "æœªçŸ¥æ–‡æ¡£") if doc.metadata else "æœªçŸ¥æ–‡æ¡£"
            content = doc.page_content.strip()
            metadata = dict(doc.metadata) if doc.metadata else {}
            items.append({
                "index": i,
                "filename": filename,
                "content": content,
                "metadata": metadata,
            })

        return json.dumps({"items": items, "count": len(items)}, ensure_ascii=False)

    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        logger.error(f"âŒ Document retrieval failed: {error_msg}")
        
        if "NoneType" in str(type(e)):
            friendly_error = "Knowledge base not initialized. Please import documents first."
        else:
            friendly_error = f"Retrieval error: {str(e)}"
        
        return json.dumps({
            "items": [], 
            "count": 0,
            "error": friendly_error
        }, ensure_ascii=False)


@tool
async def rewrite_search_query(original_query: str, context: str = "") -> str:
    """
    é‡å†™æœç´¢æŸ¥è¯¢ä»¥æ”¹è¿›æœç´¢ç»“æœã€‚
    """
    return f"{original_query} ç›¸å…³ä¿¡æ¯"
