"""
é€’å½’æ£€ç´¢å®ç°

æ ¸å¿ƒç®—æ³•ï¼š
1. åˆå§‹æŸ¥è¯¢ â†’ è·å–å€™é€‰æ–‡æ¡£ (k=initial_k)
2. è¯„ä¼°ç»“æœè´¨é‡ â†’ åˆ¤æ–­æ˜¯å¦éœ€è¦é€’å½’
3. ç”Ÿæˆå­é—®é¢˜ â†’ å¯¹å€™é€‰ç»“æœçš„ç»†åŒ–é—®é¢˜
4. é€’å½’æ£€ç´¢ â†’ é’ˆå¯¹å­é—®é¢˜å†æ¬¡æ£€ç´¢ (k=intermediate_k)
5. ç»“æœåˆå¹¶ â†’ å»é‡å¹¶é‡æ’åº â†’ è¿”å› top_n
"""

import logging
import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from app.core.langchain import langchain_manager
from app.core.config import config
from .config import RecursiveRetrieverConfig

logger = logging.getLogger(__name__)


@dataclass
class RetrievalResult:
    """å•ä¸ªæ£€ç´¢ç»“æœ"""
    content: str
    metadata: Dict[str, Any]
    relevance_score: float = 0.0
    retrieval_depth: int = 1
    retrieval_path: List[str] = field(default_factory=list)
    """æ£€ç´¢è·¯å¾„ï¼Œç”¨äºè¿½è¸ªè¿™ä¸ªç»“æœæ˜¯å¦‚ä½•è¢«æ£€ç´¢åˆ°çš„"""


@dataclass
class RecursiveRetrievalReport:
    """é€’å½’æ£€ç´¢æŠ¥å‘Š"""
    total_results: int
    final_results: int
    recursion_depth_used: int
    execution_time: float
    retrieval_tree: Dict[str, Any]
    merge_info: Dict[str, Any]


class RecursiveRetriever:
    """é€’å½’æ£€ç´¢å™¨"""
    
    def __init__(
        self,
        config: Optional[RecursiveRetrieverConfig] = None,
        vectorstore=None,
    ):
        self.config = config or RecursiveRetrieverConfig()
        self.vectorstore = vectorstore
        self.logger = logging.getLogger(__name__)
        self._retrieval_cache = {}
        # è¿½è¸ªæ£€ç´¢ç»Ÿè®¡ä¿¡æ¯
        self._total_queries = 0
        self._total_documents = 0
        self._attempted_queries = set()
        
    def set_vectorstore(self, vectorstore):
        """è®¾ç½®å‘é‡æ•°æ®åº“"""
        self.vectorstore = vectorstore
    
    def _reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self._total_queries = 0
        self._total_documents = 0
        self._attempted_queries.clear()
    
    async def retrieve(
        self,
        query: str,
        topic: Optional[str] = None,
        return_report: bool = False,
    ) -> Tuple[List[Dict[str, Any]], Optional[RecursiveRetrievalReport]]:
        """
        æ‰§è¡Œé€’å½’æ£€ç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            topic: ä¸»é¢˜/è¡¨å
            return_report: æ˜¯å¦è¿”å›è¯¦ç»†æŠ¥å‘Š
            
        Returns:
            (æœ€ç»ˆç»“æœåˆ—è¡¨, æŠ¥å‘Š) æˆ– (æœ€ç»ˆç»“æœåˆ—è¡¨, None)
        """
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self._reset_stats()
        
        start_time = datetime.now()
        
        if not self.config.enable_recursion:
            # å•å±‚æ£€ç´¢
            docs = await self._single_retrieve(query, self.config.initial_k, topic)
            results = self._docs_to_results(docs, depth=1)
        else:
            # é€’å½’æ£€ç´¢
            results, tree = await self._recursive_retrieve(
                query,
                depth=1,
                parent_query=query,
                topic=topic,
            )
        
        # æœ€ç»ˆé‡æ’åºå’Œå»é‡
        final_results = await self._merge_and_rerank(results)
        final_results = final_results[:self.config.final_k]
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        result_dicts = [
            {
                "content": r.content,
                "metadata": r.metadata,
                "relevance_score": float(r.relevance_score),
                "retrieval_depth": r.retrieval_depth,
                "retrieval_path": r.retrieval_path,
            }
            for r in final_results
        ]
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        if return_report:
            report = RecursiveRetrievalReport(
                total_results=len(results),
                final_results=len(final_results),
                recursion_depth_used=self._calculate_max_depth(results),
                execution_time=elapsed,
                retrieval_tree=tree if self.config.enable_recursion else {},
                merge_info={"strategy": self.config.merge_strategy},
            )
            return result_dicts, report
        
        return result_dicts, None
    
    async def _single_retrieve(
        self,
        query: str,
        k: int,
        topic: Optional[str] = None,
    ) -> List[Any]:
        """å•å±‚æ£€ç´¢"""
        try:
            if topic:
                try:
                    vs = await langchain_manager.async_get_vectorstore_for_table(topic)
                except Exception:
                    vs = self.vectorstore or langchain_manager.get_vectorstore()
            else:
                vs = self.vectorstore or langchain_manager.get_vectorstore()
            
            docs = await vs.asimilarity_search(query, k=k)
            return docs
        except Exception as e:
            self.logger.error(f"å•å±‚æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def _recursive_retrieve(
        self,
        query: str,
        depth: int,
        parent_query: str,
        topic: Optional[str] = None,
    ) -> Tuple[List[RetrievalResult], Dict[str, Any]]:
        """
        é€’å½’æ£€ç´¢æ ¸å¿ƒç®—æ³•
        
        Returns:
            (æ‰€æœ‰æ£€ç´¢ç»“æœ, æ£€ç´¢æ ‘)
        """
        # ğŸ›‘ ä¿æŠ¤æœºåˆ¶ 1ï¼šæ£€æŸ¥æŸ¥è¯¢æ¬¡æ•°é™åˆ¶
        if self._total_queries >= self.config.max_query_attempts:
            self.logger.warning(
                f"âš ï¸ è¾¾åˆ°æœ€å¤§æŸ¥è¯¢æ¬¡æ•°é™åˆ¶ ({self.config.max_query_attempts})ï¼Œåœæ­¢é€’å½’"
            )
            return [], {"depth": depth, "query": query, "status": "max_queries_reached"}
        
        # ğŸ›‘ ä¿æŠ¤æœºåˆ¶ 2ï¼šæ£€æŸ¥æ–‡æ¡£æ•°é‡é™åˆ¶
        if self._total_documents >= self.config.max_total_documents:
            self.logger.warning(
                f"âš ï¸ è¾¾åˆ°æœ€å¤§æ–‡æ¡£æ•°é‡é™åˆ¶ ({self.config.max_total_documents})ï¼Œåœæ­¢é€’å½’"
            )
            return [], {"depth": depth, "query": query, "status": "max_documents_reached"}
        
        # ğŸ›‘ ä¿æŠ¤æœºåˆ¶ 3ï¼šæ£€æŸ¥é‡å¤æŸ¥è¯¢ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
        query_hash = hash(query)
        if query_hash in self._attempted_queries:
            self.logger.debug(f"âš ï¸ æŸ¥è¯¢å·²å°è¯•è¿‡ï¼Œè·³è¿‡: {query[:50]}...")
            return [], {"depth": depth, "query": query, "status": "duplicate_query"}
        
        self._attempted_queries.add(query_hash)
        self._total_queries += 1
        
        # åŸºç¡€æƒ…å†µï¼šè¾¾åˆ°æœ€å¤§æ·±åº¦
        if depth > self.config.max_recursion_depth:
            return [], {"depth": depth, "query": query, "status": "max_depth_reached"}
        
        # ç¬¬1æ­¥ï¼šæ£€ç´¢æ–‡æ¡£
        k = self.config.initial_k if depth == 1 else self.config.intermediate_k
        docs = await self._single_retrieve(query, k, topic)
        
        # ç»Ÿè®¡æ–‡æ¡£æ•°é‡
        self._total_documents += len(docs)
        
        if not docs:
            return [], {"depth": depth, "query": query, "results": 0, "status": "no_results"}
        
        # è½¬æ¢ä¸ºç»“æœå¯¹è±¡
        results = self._docs_to_results(docs, depth=depth, parent_query=parent_query)
        
        # ç¬¬2æ­¥ï¼šè¯„ä¼°ç»“æœè´¨é‡
        avg_score = np.mean([r.relevance_score for r in results]) if results else 0
        
        tree = {
            "depth": depth,
            "query": query,
            "results": len(results),
            "avg_score": float(avg_score),
            "children": [],
        }
        
        # ç¬¬3æ­¥ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­é€’å½’
        should_recurse = (
            self.config.enable_recursion
            and depth < self.config.max_recursion_depth
            and avg_score < self.config.min_confidence_score
            and self._total_queries < self.config.max_query_attempts  # æ£€æŸ¥æŸ¥è¯¢é™åˆ¶
            and self._total_documents < self.config.max_total_documents  # æ£€æŸ¥æ–‡æ¡£é™åˆ¶
        )
        
        if should_recurse:
            # ç¬¬4æ­¥ï¼šç”Ÿæˆå­é—®é¢˜
            sub_questions = await self._generate_sub_questions(
                query,
                results,
                self.config.num_sub_questions,
            )
            
            # ç¬¬5æ­¥ï¼šå¯¹æ¯ä¸ªå­é—®é¢˜è¿›è¡Œé€’å½’æ£€ç´¢
            for sub_query in sub_questions:
                # å†æ¬¡æ£€æŸ¥é™åˆ¶
                if (self._total_queries >= self.config.max_query_attempts or
                    self._total_documents >= self.config.max_total_documents):
                    self.logger.info("è¾¾åˆ°èµ„æºé™åˆ¶ï¼Œåœæ­¢ç”Ÿæˆæ›´å¤šå­é—®é¢˜")
                    break
                
                sub_results, sub_tree = await self._recursive_retrieve(
                    sub_query,
                    depth=depth + 1,
                    parent_query=query,
                    topic=topic,
                )
                results.extend(sub_results)
                tree["children"].append(sub_tree)
        
        return results, tree
    
    async def _generate_sub_questions(
        self,
        original_query: str,
        current_results: List[RetrievalResult],
        num_questions: int,
    ) -> List[str]:
        """
        åŸºäºåŸå§‹æŸ¥è¯¢å’Œå½“å‰ç»“æœï¼Œç”Ÿæˆç»†åŒ–çš„å­é—®é¢˜
        
        ç­–ç•¥ï¼š
        1. ä»åŸå§‹æŸ¥è¯¢ä¸­æå–å…³é”®è¯
        2. ç»“åˆå½“å‰ç»“æœä¸­å‡ºç°çš„å…³é”®ä¿¡æ¯
        3. ç”Ÿæˆè¡¥å……æ€§çš„é—®é¢˜
        """
        try:
            # å¦‚æœæ²¡æœ‰ LLMï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•
            model = langchain_manager.get_chat_model()
            if not model:
                return self._heuristic_sub_questions(original_query)
            
            # ä½¿ç”¨ LLM ç”Ÿæˆå­é—®é¢˜
            result_text = "\n".join([r.content[:200] for r in current_results[:3]])
            
            prompt = f"""
æ ¹æ®ä»¥ä¸‹åŸå§‹æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„éƒ¨åˆ†ç»“æœï¼Œç”Ÿæˆ {num_questions} ä¸ªæ›´å…·ä½“çš„åç»­æŸ¥è¯¢é—®é¢˜ã€‚
è¿™äº›é—®é¢˜åº”è¯¥ç”¨äºæ·±åŒ–æœç´¢å’Œè·å–æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚

åŸå§‹æŸ¥è¯¢ï¼š{original_query}

å½“å‰ç»“æœæ‘˜è¦ï¼š
{result_text}

è¯·ç”Ÿæˆ {num_questions} ä¸ªé—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªã€‚é—®é¢˜åº”è¯¥ï¼š
1. é’ˆå¯¹åŸå§‹æŸ¥è¯¢çš„ä¸åŒæ–¹é¢
2. åŸºäºå½“å‰ç»“æœçš„å†…å®¹è¿›è¡Œæ·±åŒ–
3. èƒ½å¤Ÿæ‰¾åˆ°è¡¥å……ä¿¡æ¯

åªè¾“å‡ºé—®é¢˜åˆ—è¡¨ï¼Œä¸éœ€è¦å…¶ä»–å†…å®¹ï¼š
"""
            
            from langchain_core.messages import HumanMessage
            messages = [HumanMessage(content=prompt)]
            
            response = await model.ainvoke(messages)
            questions = [
                q.strip()
                for q in response.content.split("\n")
                if q.strip() and len(q.strip()) > 5
            ]
            
            return questions[:num_questions]
        
        except Exception as e:
            self.logger.warning(f"LLM å­é—®é¢˜ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•: {e}")
            return self._heuristic_sub_questions(original_query)
    
    def _heuristic_sub_questions(self, original_query: str) -> List[str]:
        """
        å¯å‘å¼å­é—®é¢˜ç”Ÿæˆ
        
        ç­–ç•¥ï¼šç»„åˆæŸ¥è¯¢çš„ä¸åŒéƒ¨åˆ†
        """
        # ç®€å•çš„å¯å‘å¼æ–¹æ³•
        words = original_query.split()
        
        sub_questions = []
        
        # å­é—®é¢˜ 1ï¼šæ·»åŠ "è¯¦ç»†"ã€"å…·ä½“"ç­‰ä¿®é¥°è¯
        if len(words) > 0:
            sub_questions.append(f"{original_query}çš„å…·ä½“ç»†èŠ‚æ˜¯ä»€ä¹ˆï¼Ÿ")
        
        # å­é—®é¢˜ 2ï¼šåå‘é—®é¢˜
        if len(words) > 1:
            sub_questions.append(f"{' '.join(words[1:])}")
        
        # ğŸ›¡ï¸ Bug Fix #5ï¼šç¡®ä¿è¿”å›å€¼ä¸è¶…è¿‡é…ç½®é™åˆ¶
        return sub_questions[:self.config.num_sub_questions]
    
    def _docs_to_results(
        self,
        docs: List[Any],
        depth: int = 1,
        parent_query: str = "",
    ) -> List[RetrievalResult]:
        """å°† LangChain æ–‡æ¡£è½¬æ¢ä¸º RetrievalResult"""
        results = []
        for i, doc in enumerate(docs):
            content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
            metadata = doc.metadata if hasattr(doc, 'metadata') else {}
            
            # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†ï¼ˆå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ï¼‰
            relevance_score = metadata.get('relevance_score', 0.5 + (10 - i) * 0.05)
            relevance_score = min(1.0, max(0.0, relevance_score))
            
            result = RetrievalResult(
                content=content,
                metadata=metadata,
                relevance_score=relevance_score,
                retrieval_depth=depth,
                retrieval_path=[parent_query] if parent_query else [],
            )
            results.append(result)
        
        return results
    
    async def _merge_and_rerank(
        self,
        results: List[RetrievalResult],
    ) -> List[RetrievalResult]:
        """
        åˆå¹¶æ¥è‡ªä¸åŒå±‚çš„ç»“æœï¼Œå»é‡ï¼Œå¹¶é‡æ’åº
        """
        if not results:
            return []
        
        # ç¬¬1æ­¥ï¼šå»é‡ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
        deduped = self._deduplicate(results)
        
        # ç¬¬2æ­¥ï¼šé‡æ’åº
        if self.config.enable_reranking and len(deduped) > 1:
            deduped = await self._rerank_results(deduped)
        
        # ç¬¬3æ­¥ï¼šæŒ‰åˆ†æ•°æ’åº
        deduped.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return deduped
    
    def _deduplicate(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """å»é™¤é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ç»“æœ"""
        if len(results) <= 1:
            return results
        
        contents = [r.content for r in results]
        
        try:
            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 2))
            tfidf_matrix = vectorizer.fit_transform(contents)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # è´ªå¿ƒå»é‡
            kept = []
            used = set()
            
            # æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
            sorted_indices = sorted(
                range(len(results)),
                key=lambda i: results[i].relevance_score,
                reverse=True,
            )
            
            for idx in sorted_indices:
                if idx in used:
                    continue
                
                kept.append(results[idx])
                
                # æ ‡è®°ç›¸ä¼¼çš„ç»“æœä¸ºå·²ä½¿ç”¨
                for other_idx in range(len(results)):
                    if other_idx != idx and similarity_matrix[idx][other_idx] > self.config.deduplication_threshold:
                        used.add(other_idx)
            
            return kept
        
        except Exception as e:
            self.logger.warning(f"å»é‡å¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ: {e}")
            return results
    
    async def _rerank_results(
        self,
        results: List[RetrievalResult],
    ) -> List[RetrievalResult]:
        """é‡æ’åºç»“æœ"""
        if self.config.rerank_method == "cosine":
            return self._rerank_cosine(results)
        elif self.config.rerank_method == "cross_encoder":
            return await self._rerank_cross_encoder(results)
        else:
            return results
    
    def _rerank_cosine(self, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦é‡æ’åº"""
        if len(results) <= 1:
            return results
        
        try:
            # ç»„åˆæ‰€æœ‰å†…å®¹
            contents = [r.content for r in results]
            
            # è¿™é‡Œåº”è¯¥æœ‰åŸå§‹æŸ¥è¯¢ï¼Œä½†æˆ‘ä»¬ç”¨ç¬¬ä¸€ä¸ªçš„è·¯å¾„ä½œä¸ºå‚è€ƒ
            primary_query = results[0].retrieval_path[0] if results[0].retrieval_path else "æŸ¥è¯¢"
            
            # å‘é‡åŒ–
            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 2), max_features=100)
            combined = [primary_query] + contents
            tfidf_matrix = vectorizer.fit_transform(combined)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            query_vector = tfidf_matrix[0:1]
            doc_vectors = tfidf_matrix[1:]
            similarities = cosine_similarity(query_vector, doc_vectors)[0]
            
            # æ›´æ–°åˆ†æ•°ï¼ˆåŠ æƒåŸæœ‰åˆ†æ•°ï¼‰
            for i, sim_score in enumerate(similarities):
                results[i].relevance_score = 0.4 * results[i].relevance_score + 0.6 * sim_score
            
            return results
        
        except Exception as e:
            self.logger.warning(f"ä½™å¼¦é‡æ’åºå¤±è´¥: {e}")
            return results
    
    async def _rerank_cross_encoder(
        self,
        results: List[RetrievalResult],
    ) -> List[RetrievalResult]:
        """ä½¿ç”¨ CrossEncoder é‡æ’åº"""
        try:
            from sentence_transformers import CrossEncoder
            
            if not hasattr(self, '_cross_encoder'):
                self._cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384')
            
            primary_query = results[0].retrieval_path[0] if results[0].retrieval_path else "æŸ¥è¯¢"
            contents = [r.content for r in results]
            
            # æ‰¹é‡è®¡åˆ†
            pairs = [[primary_query, content] for content in contents]
            scores = self._cross_encoder.predict(pairs)
            
            # æ›´æ–°åˆ†æ•°
            for i, score in enumerate(scores):
                normalized_score = 1 / (1 + np.exp(-score))  # sigmoid
                results[i].relevance_score = 0.4 * results[i].relevance_score + 0.6 * normalized_score
            
            return results
        
        except Exception as e:
            self.logger.warning(f"CrossEncoder é‡æ’åºå¤±è´¥: {e}")
            return results
    
    def _calculate_max_depth(self, results: List[RetrievalResult]) -> int:
        """è®¡ç®—ä½¿ç”¨çš„æœ€å¤§æ·±åº¦"""
        if not results:
            return 1
        # ğŸ›¡ï¸ Bug Fix #4ï¼šå¤„ç† retrieval_depth ä¸º None çš„æƒ…å†µ
        depths = [r.retrieval_depth for r in results if r.retrieval_depth is not None]
        return max(depths) if depths else 1
