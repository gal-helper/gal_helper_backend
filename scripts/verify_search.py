# -*- coding: utf-8 -*-
"""
å‘é‡å’Œå…³é”®è¯æœç´¢ Bug æ£€æŸ¥å’ŒéªŒè¯å·¥å…·
éªŒè¯æœç´¢åŠŸèƒ½çš„æ­£ç¡®æ€§ï¼Œæ£€æµ‹æ½œåœ¨é—®é¢˜
"""

import logging
import asyncio
from typing import List, Tuple, Dict, Optional
import numpy as np
from datetime import datetime

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, text

from app.models.document import Document
from app.services.retriever.hybrid_retriever import HybridRetriever

logger = logging.getLogger(__name__)


class SearchVerifier:
    """
    æœç´¢åŠŸèƒ½éªŒè¯å·¥å…·
    
    æ£€æŸ¥é¡¹ï¼š
    1. å‘é‡ç´¢å¼•å®Œæ•´æ€§
    2. å…³é”®è¯ç´¢å¼•å®Œæ•´æ€§
    3. å‘é‡ç›¸ä¼¼åº¦è®¡ç®—æ­£ç¡®æ€§
    4. å…³é”®è¯åŒ¹é…æ­£ç¡®æ€§
    5. æ ‡ç­¾è¿‡æ»¤åŠŸèƒ½
    """
    
    def __init__(self, db_session: AsyncSession):
        self.db = db_session
        self.results = {}
    
    async def verify_all(self) -> Dict[str, bool]:
        """æ‰§è¡Œæ‰€æœ‰éªŒè¯"""
        
        logger.info("ğŸ” å¼€å§‹æœç´¢åŠŸèƒ½éªŒè¯...")
        
        # 1. æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
        await self._verify_database_health()
        
        # 2. æ£€æŸ¥å‘é‡ç´¢å¼•
        await self._verify_vector_index()
        
        # 3. æ£€æŸ¥å…³é”®è¯ç´¢å¼•
        await self._verify_keyword_index()
        
        # 4. æ£€æŸ¥æ ‡ç­¾ç³»ç»Ÿ
        await self._verify_tag_system()
        
        # 5. æ‰§è¡Œæœç´¢æµ‹è¯•
        await self._verify_search_functionality()
        
        # 6. æ€§èƒ½æµ‹è¯•
        await self._verify_performance()
        
        logger.info("âœ… éªŒè¯å®Œæˆ")
        return self.results
    
    async def _verify_database_health(self):
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
        
        logger.info("ğŸ“Š æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€...")
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            result = await self.db.execute(text("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = 'ai_documents'
                )
            """))
            
            exists = result.scalar()
            self.results["database_table_exists"] = exists
            
            if exists:
                logger.info("âœ… ai_documents è¡¨å­˜åœ¨")
            else:
                logger.error("âŒ ai_documents è¡¨ä¸å­˜åœ¨")
                return
            
            # æ£€æŸ¥æ–‡æ¡£æ•°é‡
            result = await self.db.execute(select(func.count(Document.id)))
            doc_count = result.scalar()
            
            logger.info(f"ğŸ“ˆ æ–‡æ¡£æ•°é‡: {doc_count}")
            self.results["document_count"] = doc_count
            
            if doc_count == 0:
                logger.warning("âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£ï¼Œéƒ¨åˆ†æµ‹è¯•æ— æ³•æ‰§è¡Œ")
                return
            
            # æ£€æŸ¥å·²ç´¢å¼•æ–‡æ¡£æ¯”ä¾‹
            result = await self.db.execute(
                select(func.count(Document.id)).where(Document.is_indexed == True)
            )
            indexed_count = result.scalar()
            
            index_rate = (indexed_count / doc_count * 100) if doc_count > 0 else 0
            logger.info(f"ğŸ” å·²ç´¢å¼•æ–‡æ¡£: {indexed_count}/{doc_count} ({index_rate:.1f}%)")
            self.results["indexed_rate"] = index_rate
            
            # æ£€æŸ¥å·²æ ‡ç­¾åŒ–æ–‡æ¡£æ¯”ä¾‹
            result = await self.db.execute(
                select(func.count(Document.id)).where(Document.is_tagged == True)
            )
            tagged_count = result.scalar()
            
            tag_rate = (tagged_count / doc_count * 100) if doc_count > 0 else 0
            logger.info(f"ğŸ·ï¸  å·²æ ‡ç­¾åŒ–æ–‡æ¡£: {tagged_count}/{doc_count} ({tag_rate:.1f}%)")
            self.results["tagged_rate"] = tag_rate
        
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.results["database_health"] = False
    
    async def _verify_vector_index(self):
        """éªŒè¯å‘é‡ç´¢å¼•"""
        
        logger.info("ğŸ” éªŒè¯å‘é‡ç´¢å¼•...")
        
        try:
            # æ£€æŸ¥å‘é‡åˆ—çš„æ•°æ®ç±»å‹
            result = await self.db.execute(text("""
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns
                WHERE table_name = 'ai_documents' AND column_name = 'embedding'
            """))
            
            row = result.fetchone()
            if row:
                col_name, data_type, nullable = row
                logger.info(f"âœ… å‘é‡åˆ—æ‰¾åˆ°: {col_name} ({data_type})")
                self.results["vector_column_exists"] = True
                
                if "vector" not in data_type.lower():
                    logger.warning(f"âš ï¸  å‘é‡åˆ—ç±»å‹å¯èƒ½ä¸å¯¹: {data_type}")
            else:
                logger.error("âŒ å‘é‡åˆ—ä¸å­˜åœ¨")
                self.results["vector_column_exists"] = False
                return
            
            # æ£€æŸ¥å‘é‡ç´¢å¼•
            result = await self.db.execute(text("""
                SELECT indexname, indexdef
                FROM pg_indexes
                WHERE tablename = 'ai_documents' AND indexname LIKE '%embedding%'
            """))
            
            indexes = result.fetchall()
            logger.info(f"ğŸ” å‘é‡ç´¢å¼•æ•°: {len(indexes)}")
            for idx_name, idx_def in indexes:
                logger.info(f"   - {idx_name}")
            
            self.results["vector_index_count"] = len(indexes)
            
            # æ£€æŸ¥æœ‰å‘é‡çš„æ–‡æ¡£
            result = await self.db.execute(
                select(func.count(Document.id)).where(Document.embedding != None)
            )
            vec_count = result.scalar()
            logger.info(f"ğŸ“Š æœ‰å‘é‡çš„æ–‡æ¡£: {vec_count}")
            self.results["documents_with_vectors"] = vec_count
            
            # æ£€æŸ¥å‘é‡ç»´åº¦
            result = await self.db.execute(text("""
                SELECT dimension FROM (
                    SELECT array_length(embedding::float4[], 1) AS dimension
                    FROM ai_documents
                    WHERE embedding IS NOT NULL
                    LIMIT 1
                ) sub
            """))
            
            row = result.fetchone()
            if row:
                dim = row[0]
                logger.info(f"ğŸ“ å‘é‡ç»´åº¦: {dim}")
                self.results["vector_dimension"] = dim
        
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç´¢å¼•éªŒè¯å¤±è´¥: {e}")
            self.results["vector_index_verified"] = False
    
    async def _verify_keyword_index(self):
        """éªŒè¯å…³é”®è¯ç´¢å¼•"""
        
        logger.info("ğŸ” éªŒè¯å…³é”®è¯ç´¢å¼•...")
        
        try:
            # æ£€æŸ¥å…³é”®è¯åˆ—
            result = await self.db.execute(text("""
                SELECT column_name, data_type
                FROM information_schema.columns
                WHERE table_name = 'ai_documents' AND column_name = 'keywords'
            """))
            
            row = result.fetchone()
            if row:
                col_name, data_type = row
                logger.info(f"âœ… å…³é”®è¯åˆ—æ‰¾åˆ°: {col_name} ({data_type})")
                self.results["keyword_column_exists"] = True
            else:
                logger.error("âŒ å…³é”®è¯åˆ—ä¸å­˜åœ¨")
                self.results["keyword_column_exists"] = False
                return
            
            # æ£€æŸ¥å…³é”®è¯ç´¢å¼•
            result = await self.db.execute(text("""
                SELECT indexname, indexdef
                FROM pg_indexes
                WHERE tablename = 'ai_documents' AND indexname LIKE '%keyword%'
            """))
            
            indexes = result.fetchall()
            logger.info(f"ğŸ” å…³é”®è¯ç´¢å¼•æ•°: {len(indexes)}")
            self.results["keyword_index_count"] = len(indexes)
            
            # æ£€æŸ¥æœ‰å…³é”®è¯çš„æ–‡æ¡£
            result = await self.db.execute(text("""
                SELECT COUNT(*) FROM ai_documents WHERE keywords IS NOT NULL AND array_length(keywords, 1) > 0
            """))
            
            kw_count = result.scalar()
            logger.info(f"ğŸ“Š æœ‰å…³é”®è¯çš„æ–‡æ¡£: {kw_count}")
            self.results["documents_with_keywords"] = kw_count
            
            # æ£€æŸ¥å…³é”®è¯è¦†ç›–ç‡
            result = await self.db.execute(select(func.count(Document.id)))
            total_count = result.scalar()
            
            if total_count > 0:
                kw_rate = (kw_count / total_count * 100)
                logger.info(f"ğŸ“ˆ å…³é”®è¯è¦†ç›–ç‡: {kw_rate:.1f}%")
                self.results["keyword_coverage"] = kw_rate
        
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯ç´¢å¼•éªŒè¯å¤±è´¥: {e}")
            self.results["keyword_index_verified"] = False
    
    async def _verify_tag_system(self):
        """éªŒè¯æ ‡ç­¾ç³»ç»Ÿ"""
        
        logger.info("ğŸ” éªŒè¯æ ‡ç­¾ç³»ç»Ÿ...")
        
        try:
            # æ£€æŸ¥æ ‡ç­¾åˆ—
            result = await self.db.execute(text("""
                SELECT column_name, data_type
                FROM information_schema.columns
                WHERE table_name = 'ai_documents' AND column_name = 'tags'
            """))
            
            row = result.fetchone()
            if row:
                col_name, data_type = row
                logger.info(f"âœ… æ ‡ç­¾åˆ—æ‰¾åˆ°: {col_name} ({data_type})")
                self.results["tag_column_exists"] = True
            else:
                logger.error("âŒ æ ‡ç­¾åˆ—ä¸å­˜åœ¨")
                self.results["tag_column_exists"] = False
                return
            
            # æ£€æŸ¥æœ‰æ ‡ç­¾çš„æ–‡æ¡£
            result = await self.db.execute(text("""
                SELECT COUNT(*) FROM ai_documents 
                WHERE tags IS NOT NULL AND tags::text != '{}'
            """))
            
            tag_count = result.scalar()
            logger.info(f"ğŸ“Š æœ‰æ ‡ç­¾çš„æ–‡æ¡£: {tag_count}")
            self.results["documents_with_tags"] = tag_count
            
            # æ£€æŸ¥æ ‡ç­¾ç¼“å­˜è¡¨
            result = await self.db.execute(text("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = 'ai_document_tag_cache'
                )
            """))
            
            cache_exists = result.scalar()
            logger.info(f"{'âœ…' if cache_exists else 'âš ï¸'} æ ‡ç­¾ç¼“å­˜è¡¨: {'å­˜åœ¨' if cache_exists else 'ä¸å­˜åœ¨'}")
            self.results["tag_cache_exists"] = cache_exists
        
        except Exception as e:
            logger.error(f"âŒ æ ‡ç­¾ç³»ç»ŸéªŒè¯å¤±è´¥: {e}")
            self.results["tag_system_verified"] = False
    
    async def _verify_search_functionality(self):
        """éªŒè¯æœç´¢åŠŸèƒ½"""
        
        logger.info("ğŸ” éªŒè¯æœç´¢åŠŸèƒ½...")
        
        try:
            # è·å–ä¸€ä¸ªæœ‰å‘é‡çš„æ–‡æ¡£ç”¨ä½œæµ‹è¯•
            result = await self.db.execute(
                select(Document).where(Document.embedding != None).limit(1)
            )
            test_doc = result.scalar_one_or_none()
            
            if not test_doc:
                logger.warning("âš ï¸  æ²¡æœ‰æœ‰å‘é‡çš„æ–‡æ¡£ï¼Œè·³è¿‡æœç´¢æµ‹è¯•")
                self.results["search_test_skipped"] = True
                return
            
            logger.info(f"ğŸ§ª ä½¿ç”¨æ–‡æ¡£æµ‹è¯•æœç´¢: {test_doc.title}")
            
            # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
            retriever = HybridRetriever(self.db)
            
            # æµ‹è¯•å‘é‡æœç´¢
            try:
                vector_results = await retriever._vector_search(
                    embedding=test_doc.embedding,
                    top_k=5
                )
                logger.info(f"âœ… å‘é‡æœç´¢æˆåŠŸ: {len(vector_results)} ç»“æœ")
                self.results["vector_search_works"] = True
            except Exception as e:
                logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}")
                self.results["vector_search_works"] = False
            
            # æµ‹è¯•å…³é”®è¯æœç´¢
            try:
                keyword_results = await retriever._keyword_search(
                    query=test_doc.title,
                    top_k=5
                )
                logger.info(f"âœ… å…³é”®è¯æœç´¢æˆåŠŸ: {len(keyword_results)} ç»“æœ")
                self.results["keyword_search_works"] = True
            except Exception as e:
                logger.error(f"âŒ å…³é”®è¯æœç´¢å¤±è´¥: {e}")
                self.results["keyword_search_works"] = False
            
            # æµ‹è¯•æ··åˆæœç´¢
            try:
                hybrid_results = await retriever.hybrid_search(
                    query=test_doc.title,
                    embedding=test_doc.embedding,
                    top_k=5
                )
                logger.info(f"âœ… æ··åˆæœç´¢æˆåŠŸ: {len(hybrid_results)} ç»“æœ")
                self.results["hybrid_search_works"] = True
            except Exception as e:
                logger.error(f"âŒ æ··åˆæœç´¢å¤±è´¥: {e}")
                self.results["hybrid_search_works"] = False
        
        except Exception as e:
            logger.error(f"âŒ æœç´¢åŠŸèƒ½éªŒè¯å¤±è´¥: {e}")
            self.results["search_functionality_verified"] = False
    
    async def _verify_performance(self):
        """æ€§èƒ½æµ‹è¯•"""
        
        logger.info("â±ï¸  æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
        
        try:
            # æµ‹è¯•å‘é‡æœç´¢é€Ÿåº¦
            result = await self.db.execute(
                select(Document).where(Document.embedding != None).limit(1)
            )
            test_doc = result.scalar_one_or_none()
            
            if test_doc:
                start_time = datetime.now()
                result = await self.db.execute(text("""
                    SELECT id FROM ai_documents 
                    WHERE embedding IS NOT NULL 
                    ORDER BY embedding <-> %s LIMIT 10
                """), [test_doc.embedding])
                
                end_time = datetime.now()
                elapsed = (end_time - start_time).total_seconds() * 1000
                
                logger.info(f"â±ï¸  å‘é‡æœç´¢è€—æ—¶: {elapsed:.2f}ms")
                self.results["vector_search_latency_ms"] = elapsed
                
                if elapsed < 200:
                    logger.info("âœ… å‘é‡æœç´¢æ€§èƒ½è‰¯å¥½")
                else:
                    logger.warning("âš ï¸  å‘é‡æœç´¢æ€§èƒ½å¯ä»¥ä¼˜åŒ–")
        
        except Exception as e:
            logger.debug(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")


async def run_verification(db_session: AsyncSession):
    """è¿è¡Œå®Œæ•´çš„éªŒè¯"""
    
    verifier = SearchVerifier(db_session)
    results = await verifier.verify_all()
    
    logger.info("\n" + "="*50)
    logger.info("éªŒè¯ç»“æœæ±‡æ€»")
    logger.info("="*50)
    
    passed = sum(1 for v in results.values() if v is True)
    total = len(results)
    
    logger.info(f"é€šè¿‡: {passed}/{total} é¡¹éªŒè¯")
    
    for key, value in results.items():
        status = "âœ…" if value is True else "âš ï¸" if value is None else "âŒ"
        logger.info(f"{status} {key}: {value}")
    
    return results


# å¯¼å…¥ func
from sqlalchemy import func
