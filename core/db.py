import asyncpg
from psycopg_pool import AsyncConnectionPool
from sqlalchemy.ext.asyncio import (
    async_sessionmaker,
    AsyncSession,
    create_async_engine,
    AsyncEngine,
)
from sqlalchemy import text
from typing import Optional, AsyncGenerator
from contextlib import asynccontextmanager
from app.core.config import config
import logging

logger = logging.getLogger(__name__)


class AsyncDatabaseManager:
    def __init__(self):
        self.async_engine: Optional[AsyncEngine] = None
        self._async_session: Optional[async_sessionmaker[AsyncSession]] = None
        self._initialized = False

    async def init_async_database(self):
        if self._initialized:
            return

        logger.info("Initializing async database...")

        # ç›´æ¥ä½¿ç”¨ config.ASYNC_DATABASE_URL
        database_url = config.ASYNC_DATABASE_URL
        logger.info(f"Using database URL: {database_url}")

        # åˆ›å»ºå¼‚æ­¥å¼•æ“
        self.async_engine = create_async_engine(
            database_url,
            echo=config.DEBUG,
            pool_size=config.DB_POOL_MIN_SIZE,
            max_overflow=config.DB_POOL_MAX_SIZE - config.DB_POOL_MIN_SIZE,
            pool_pre_ping=True,
        )

        # æµ‹è¯•è¿æ¥
        try:
            async with self.async_engine.connect() as conn:
                await conn.execute(text("SELECT 1"))
                await conn.commit()
            logger.info("âœ… Database connection successful")
        except Exception as e:
            logger.error(f"âŒ Database connection failed: {e}")
            raise

        # åˆ›å»ºå¼‚æ­¥ä¼šè¯å·¥å‚
        self._async_session = async_sessionmaker(
            bind=self.async_engine,
            class_=AsyncSession,
            expire_on_commit=False,
            autocommit=False,
            autoflush=False,
        )

        self._initialized = True
        logger.info("Async database initialized successfully.")

    async def close(self):
        """å…³é—­å¼‚æ­¥æ•°æ®åº“å¼•æ“å’Œè¿æ¥æ± """
        if self.async_engine:
            logger.info("Closing async database engine...")
            await self.async_engine.dispose()
            self.async_engine = None
            self._async_session = None
            self._initialized = False
            logger.info("Async database engine disposed.")

    @asynccontextmanager
    async def get_async_db(self) -> AsyncGenerator[AsyncSession, None]:
        """è·å–æ•°æ®åº“ä¼šè¯çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if not self._initialized:
            await self.init_async_database()

        async with self._async_session() as session:
            try:
                yield session
                await session.commit()
            except Exception as e:
                await session.rollback()
                raise
            finally:
                await session.close()


# åˆ›å»ºå…¨å±€å”¯ä¸€å®ä¾‹
async_db_manager = AsyncDatabaseManager()


class LangchainConnectionPool:
    """LangGraph ä¸“ç”¨çš„è¿æ¥æ± ç®¡ç†"""

    def __init__(self):
        self._pool: Optional[AsyncConnectionPool] = None
        self._initialized = False

    async def connect(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        if self._initialized:
            return

        logger.info("Initializing Langchain connection pool...")

        # ç›´æ¥ä½¿ç”¨ config.LANGCHAIN_DATABASE_URL
        database_url = config.LANGCHAIN_DATABASE_URL
        logger.info(f"Using Langchain database URL: {database_url}")

        # åˆ›å»ºè¿æ¥æ± 
        self._pool = AsyncConnectionPool(
            database_url,
            min_size=config.DB_POOL_MIN_SIZE,
            max_size=config.DB_POOL_MAX_SIZE,
            open=False,
            timeout=30,
            max_idle=300,
            kwargs={
                "application_name": "langchain_rag",
                "options": "-c statement_timeout=30000",
            }
        )

        # æ‰‹åŠ¨æ‰“å¼€è¿æ¥æ± 
        await self._pool.open()

        # éªŒè¯è¿æ¥æ± 
        try:
            async with self._pool.connection() as conn:
                async with conn.cursor() as cur:
                    await cur.execute("SELECT 1")
            logger.info("âœ… Langchain connection pool ready")
        except Exception as e:
            logger.error(f"âŒ Langchain connection pool validation failed: {e}")
            await self._pool.close()
            raise

        self._initialized = True
        logger.info("Langchain connection pool initialized successfully.")

    async def disconnect(self):
        """å…³é—­è¿æ¥æ± """
        if self._pool and self._initialized:
            logger.info("Closing Langchain connection pool...")
            await self._pool.close()
            self._pool = None
            self._initialized = False
            logger.info("Langchain connection pool closed.")

    def get_pool(self) -> AsyncConnectionPool:
        """è·å–è¿æ¥æ± å®ä¾‹"""
        if not self._pool or not self._initialized:
            raise RuntimeError(
                "Langchain connection pool not initialized. Call connect() first."
            )
        return self._pool


langchain_pool = LangchainConnectionPool()


class DatabaseInitializer:
    """æ•°æ®åº“åˆå§‹åŒ–å™¨ï¼Œå¤„ç†è¡¨åˆ›å»ºå’Œç´¢å¼•ç®¡ç†"""

    def __init__(self):
        self._initialized = False

    async def ensure_vector_extension(self, conn):
        """ç¡®ä¿ vector æ‰©å±•å·²å®‰è£…"""
        await conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
        logger.debug("Vector extension ensured")

    async def ensure_vector_table(self, conn):
        """ç¡®ä¿å‘é‡è¡¨å­˜åœ¨"""
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        result = await conn.execute(text("""
                                         SELECT EXISTS (SELECT
                                                        FROM information_schema.tables
                                                        WHERE table_name = 'document_embeddings')
                                         """))
        row = result.fetchone()
        table_exists = row[0] if row else False

        if not table_exists:
            logger.info("Creating document_embeddings table...")
            await conn.execute(text(f"""
                CREATE TABLE document_embeddings (
                    id BIGSERIAL PRIMARY KEY,
                    langchain_id TEXT,
                    document_content TEXT,
                    embedding vector({config.VECTOR_DIMENSION}),
                    langchain_metadata JSONB,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                )
            """))
            logger.info("âœ… Table document_embeddings created")
        else:
            logger.debug("Table document_embeddings already exists")

    async def create_vector_index(self):
        """åˆ›å»ºå‘é‡ç´¢å¼• - ä½¿ç”¨ asyncpg ç›´æ¥è¿æ¥"""
        if config.SKIP_INDEX_CREATION:
            logger.info("Index creation skipped by configuration")
            return

        conn = None
        try:
            # ç›´æ¥ä»è¿æ¥å­—ç¬¦ä¸²åˆ›å»ºè¿æ¥ï¼Œå®Œå…¨ç»•è¿‡è¿æ¥æ± 
            conn = await asyncpg.connect(config.LANGCHAIN_DATABASE_URL)

            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            row = await conn.fetchrow("""
                                      SELECT 1
                                      FROM pg_indexes
                                      WHERE indexname = 'document_embeddings_embedding_idx'
                                      """)
            index_exists = row is not None

            if not index_exists and config.AUTO_CREATE_INDEX:
                logger.info("ğŸ”¨ Creating vector index (this may take a while)...")

                # è®¾ç½®ç´¢å¼•åˆ›å»ºè¶…æ—¶
                await conn.execute("SET statement_timeout = '5min'")

                # æ‰§è¡Œ CONCURRENTLY åˆ›å»ºç´¢å¼•
                await conn.execute(f"""
                    CREATE INDEX CONCURRENTLY document_embeddings_embedding_idx
                        ON document_embeddings
                        USING ivfflat (embedding vector_cosine_ops)
                        WITH (lists = {config.VECTOR_INDEX_LISTS})
                """)
                logger.info("âœ… Vector index created successfully")
            else:
                logger.info("âœ… Vector index already exists")

        except Exception as e:
            logger.error(f"Index creation failed: {e}")
            logger.info("You can create it manually later with:")
            logger.info(
                f"  CREATE INDEX CONCURRENTLY document_embeddings_embedding_idx ON document_embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = {config.VECTOR_INDEX_LISTS});")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç³»ç»Ÿç»§ç»­è¿è¡Œ
        finally:
            # ç¡®ä¿è¿æ¥è¢«å…³é—­
            if conn and not conn.is_closed():
                await conn.close()

    async def initialize(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®åº“åˆå§‹åŒ–"""
        if self._initialized:
            return

        logger.info("=" * 60)
        logger.info("Starting database initialization")
        logger.info("=" * 60)

        try:
            # 1. ç¡®ä¿è¿æ¥æ± å·²åˆå§‹åŒ–ï¼ˆç”¨äºåç»­æ“ä½œï¼‰
            await langchain_pool.connect()

            # 2. åœ¨äº‹åŠ¡å†…åˆ›å»ºè¡¨ç»“æ„
            async with async_db_manager.async_engine.begin() as conn:
                await self.ensure_vector_extension(conn)
                await self.ensure_vector_table(conn)

            logger.info("âœ… Database schema initialized")

            # 3. åœ¨äº‹åŠ¡å¤–åˆ›å»ºç´¢å¼• - ä½¿ç”¨ç‹¬ç«‹çš„ asyncpg è¿æ¥
            await self.create_vector_index()

            self._initialized = True
            logger.info("=" * 60)
            logger.info("Database initialization completed")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"Database initialization failed: {e}")
            raise


# åˆ›å»ºå…¨å±€åˆå§‹åŒ–å™¨
db_initializer = DatabaseInitializer()